From d0583e0fc373b6e6e886c875354fdf0ec93adfeb Mon Sep 17 00:00:00 2001
From: Alejandro Abdelnur <tucu@apache.org>
Date: Mon, 9 Jul 2012 19:20:37 +0000
Subject: [PATCH 1289/1344] MAPREDUCE-3993. Graceful handling of codec errors during decompression (kkambatl via tucu)

git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/branch-1@1359348 13f79535-47bb-0310-9956-ffa450edef68
(cherry picked from commit e821891bfea977da78ad86b4ccb08582aa8f7ddd)
---
 src/core/org/apache/hadoop/io/IOUtils.java         |   23 +++++++++++++++++++-
 src/mapred/org/apache/hadoop/mapred/IFile.java     |    4 ++-
 .../org/apache/hadoop/mapred/ReduceTask.java       |    7 +++--
 3 files changed, 29 insertions(+), 5 deletions(-)

diff --git a/src/core/org/apache/hadoop/io/IOUtils.java b/src/core/org/apache/hadoop/io/IOUtils.java
index 9771c6b..fc14e84 100644
--- a/src/core/org/apache/hadoop/io/IOUtils.java
+++ b/src/core/org/apache/hadoop/io/IOUtils.java
@@ -185,7 +185,28 @@ public class IOUtils {
   }
 
   /**
-   * Reads len bytes in a loop.
+   * Utility wrapper for reading from {@link InputStream}. It catches any errors
+   * thrown by the underlying stream (either IO or decompression-related), and
+   * re-throws as an IOException.
+   * 
+   * @param is - InputStream to be read from
+   * @param buf - buffer the data is read into
+   * @param off - offset within buf
+   * @param len - amount of data to be read
+   * @return number of bytes read
+   */
+  public static int wrappedReadForCompressedData(InputStream is, byte[] buf,
+      int off, int len) throws IOException {
+    try {
+      return is.read(buf, off, len);
+    } catch (IOException ie) {
+      throw ie;
+    } catch (Throwable t) {
+      throw new IOException("Error while reading compressed data", t);
+    }
+  }
+
+  /** Reads len bytes in a loop.
    * @param in The InputStream to read from
    * @param buf The buffer to fill
    * @param off offset from the buffer
diff --git a/src/mapred/org/apache/hadoop/mapred/IFile.java b/src/mapred/org/apache/hadoop/mapred/IFile.java
index f33a7e8..38579cb 100644
--- a/src/mapred/org/apache/hadoop/mapred/IFile.java
+++ b/src/mapred/org/apache/hadoop/mapred/IFile.java
@@ -30,6 +30,7 @@ import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.io.DataInputBuffer;
 import org.apache.hadoop.io.DataOutputBuffer;
+import org.apache.hadoop.io.IOUtils;
 import org.apache.hadoop.io.WritableUtils;
 import org.apache.hadoop.io.compress.CodecPool;
 import org.apache.hadoop.io.compress.CompressionCodec;
@@ -338,7 +339,8 @@ class IFile {
     private int readData(byte[] buf, int off, int len) throws IOException {
       int bytesRead = 0;
       while (bytesRead < len) {
-        int n = in.read(buf, off+bytesRead, len-bytesRead);
+        int n = IOUtils.wrappedReadForCompressedData(in, buf, off + bytesRead,
+            len - bytesRead);
         if (n < 0) {
           return bytesRead;
         }
diff --git a/src/mapred/org/apache/hadoop/mapred/ReduceTask.java b/src/mapred/org/apache/hadoop/mapred/ReduceTask.java
index 1140a93..d7bf6a1 100644
--- a/src/mapred/org/apache/hadoop/mapred/ReduceTask.java
+++ b/src/mapred/org/apache/hadoop/mapred/ReduceTask.java
@@ -1616,15 +1616,16 @@ class ReduceTask extends Task {
         
         int bytesRead = 0;
         try {
-          int n = input.read(shuffleData, 0, shuffleData.length);
+          int n = IOUtils.wrappedReadForCompressedData(input, shuffleData, 0,
+              shuffleData.length);
           while (n > 0) {
             bytesRead += n;
             shuffleClientMetrics.inputBytes(n);
 
             // indicate we're making progress
             reporter.progress();
-            n = input.read(shuffleData, bytesRead, 
-                           (shuffleData.length-bytesRead));
+            n = IOUtils.wrappedReadForCompressedData(input, shuffleData,
+                bytesRead, shuffleData.length - bytesRead);
           }
 
           if (LOG.isDebugEnabled()) {
-- 
1.7.0.4

