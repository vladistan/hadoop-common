From 7f45d832d8277abb4c18079101085588fcfc8553 Mon Sep 17 00:00:00 2001
From: Harsh J <harsh@cloudera.com>
Date: Mon, 9 Jul 2012 11:59:40 +0530
Subject: [PATCH 1292/1344] HDFS-96. HDFS does not support blocks greater than 2GB

Reason: Community Request
Ref: CDH-6463
Author: Patrick Kling
---
 src/hdfs/org/apache/hadoop/hdfs/DFSClient.java     |    2 +-
 .../hadoop/hdfs/server/datanode/BlockSender.java   |    4 +-
 .../org/apache/hadoop/hdfs/TestLargeBlock.java     |  188 ++++++++++++++++++++
 3 files changed, 191 insertions(+), 3 deletions(-)
 create mode 100644 src/test/org/apache/hadoop/hdfs/TestLargeBlock.java

diff --git a/src/hdfs/org/apache/hadoop/hdfs/DFSClient.java b/src/hdfs/org/apache/hadoop/hdfs/DFSClient.java
index cfd3580..520a636 100644
--- a/src/hdfs/org/apache/hadoop/hdfs/DFSClient.java
+++ b/src/hdfs/org/apache/hadoop/hdfs/DFSClient.java
@@ -2315,7 +2315,7 @@ public class DFSClient implements FSConstants, java.io.Closeable {
             if (pos > blockEnd) {
               currentNode = blockSeekTo(pos);
             }
-            int realLen = Math.min(len, (int) (blockEnd - pos + 1));
+            int realLen = (int) Math.min((long) len, (blockEnd - pos + 1L));
             int result = readBuffer(buf, off, realLen);
             
             if (result >= 0) {
diff --git a/src/hdfs/org/apache/hadoop/hdfs/server/datanode/BlockSender.java b/src/hdfs/org/apache/hadoop/hdfs/server/datanode/BlockSender.java
index df796c1..857f438 100644
--- a/src/hdfs/org/apache/hadoop/hdfs/server/datanode/BlockSender.java
+++ b/src/hdfs/org/apache/hadoop/hdfs/server/datanode/BlockSender.java
@@ -283,8 +283,8 @@ class BlockSender implements java.io.Closeable, FSConstants {
                          throws IOException {
     // Sends multiple chunks in one packet with a single write().
 
-    int len = Math.min((int) (endOffset - offset),
-                       bytesPerChecksum*maxChunks);
+    int len = (int) Math.min(endOffset - offset,
+        (((long) bytesPerChecksum) * ((long) maxChunks)));
     
     // truncate len so that any partial chunks will be sent as a final packet.
     // this is not necessary for correctness, but partial chunks are 
diff --git a/src/test/org/apache/hadoop/hdfs/TestLargeBlock.java b/src/test/org/apache/hadoop/hdfs/TestLargeBlock.java
new file mode 100644
index 0000000..a7bcab1
--- /dev/null
+++ b/src/test/org/apache/hadoop/hdfs/TestLargeBlock.java
@@ -0,0 +1,188 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hdfs;
+
+import java.io.IOException;
+import java.util.Arrays;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.fs.FSDataInputStream;
+import org.apache.hadoop.fs.FSDataOutputStream;
+import org.apache.hadoop.fs.FileSystem;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hdfs.server.datanode.SimulatedFSDataset;
+import org.junit.Test;
+
+/**
+ * This class tests that blocks can be larger than 2GB
+ */
+public class TestLargeBlock extends junit.framework.TestCase {
+  static final String DIR = "/" + TestLargeBlock.class.getSimpleName() + "/";
+
+  //should we verify the data read back from the file? (slow)
+  static final boolean verifyData = true;
+
+  static final byte[] pattern = { 'D', 'E', 'A', 'D', 'B', 'E', 'E', 'F'};
+  static final boolean simulatedStorage = false;
+
+  // creates a file
+  static FSDataOutputStream createFile(FileSystem fileSys, Path name, int repl, final long blockSize)
+    throws IOException {
+    FSDataOutputStream stm = fileSys.create(name, true,
+                                            fileSys.getConf().getInt("io.file.buffer.size", 4096),
+                                            (short)repl, blockSize);
+    System.out.println("createFile: Created " + name + " with " + repl + " replica.");
+    return stm;
+  }
+
+
+  /**
+   * Writes pattern to file
+   */
+  static void writeFile(FSDataOutputStream stm, final long fileSize) throws IOException {
+    final int writeSize = pattern.length * 8 * 1024 * 1024; // write in chunks of 64 MB
+
+    if (writeSize > Integer.MAX_VALUE) {
+      throw new IOException("A single write is too large " + writeSize);
+    }
+
+    long bytesToWrite = fileSize;
+    byte[] b = new byte[writeSize];
+
+    // initialize buffer
+    for (int j = 0; j < writeSize; j++) {
+      b[j] = pattern[j % pattern.length];
+    }
+
+    while (bytesToWrite > 0) {
+      // how many bytes we are writing in this iteration
+      int thiswrite = (int) Math.min(writeSize, bytesToWrite);
+
+      stm.write(b, 0, thiswrite);
+      bytesToWrite -= thiswrite;
+    }
+  }
+
+  /**
+   * Reads from file and makes sure that it matches the pattern
+   */
+  static void checkFullFile(FileSystem fs, Path name, final long fileSize) throws IOException {
+    final int readSize = pattern.length * 16 * 1024 * 1024; // read in chunks of 128 MB
+
+    if (readSize > Integer.MAX_VALUE) {
+      throw new IOException("A single read is too large " + readSize);
+    }
+
+    byte[] b = new byte[readSize];
+    long bytesToRead = fileSize;
+
+    byte[] compb = new byte[readSize]; // buffer with correct data for comparison
+
+    if (verifyData) {
+      // initialize compare buffer
+      for (int j = 0; j < readSize; j++) {
+        compb[j] = pattern[j % pattern.length];
+      }
+    }
+
+
+    FSDataInputStream stm = fs.open(name);
+
+    while (bytesToRead > 0) {
+      int thisread = (int) Math.min(readSize, bytesToRead); // how many bytes we are reading in this iteration
+
+      stm.readFully(b, 0, thisread);
+
+      if (verifyData) {
+        // verify data read
+
+        if (thisread == readSize) {
+          assertTrue("file corrupted at or after byte " + (fileSize - bytesToRead), Arrays.equals(b, compb));
+        } else {
+          // b was only partially filled by last read
+          for (int k = 0; k < thisread; k++) {
+            assertTrue("file corrupted at or after byte " + (fileSize - bytesToRead), b[k] == compb[k]);
+          }
+        }
+      }
+
+      // System.out.println("Read[" + i + "/" + readCount + "] " + thisread + " bytes.");
+
+      bytesToRead -= thisread;
+    }
+    stm.close();
+  }
+
+  /**
+   * Test for block size of 2GB + 512B
+   */
+  @Test
+  public void testLargeBlockSize() throws IOException {
+    final long blockSize = 2L * 1024L * 1024L * 1024L + 512L; // 2GB + 512B
+    runTest(blockSize);
+  }
+
+  /**
+   * Test that we can write to and read from large blocks
+   */
+  public void runTest(final long blockSize) throws IOException {
+
+    // write a file that is slightly larger than 1 block
+    final long fileSize = blockSize + 1L;
+
+    Configuration conf = new Configuration();
+    if (simulatedStorage) {
+      conf.setBoolean(SimulatedFSDataset.CONFIG_PROPERTY_SIMULATED, true);
+    }
+    MiniDFSCluster cluster = new MiniDFSCluster(conf, 1, true, null);
+    FileSystem fs = cluster.getFileSystem();
+    try {
+
+      // create a new file in test data directory
+      Path file1 = new Path(System.getProperty("test.build.data") + "/" + Long.toString(blockSize) + ".dat");
+      FSDataOutputStream stm = createFile(fs, file1, 1, blockSize);
+      System.out.println("File " + file1 + " created with file size " +
+                         fileSize +
+                         " blocksize " + blockSize);
+
+      // verify that file exists in FS namespace
+      assertTrue(file1 + " should be a file and not a dir",
+                  !fs.getFileStatus(file1).isDir());
+
+      // write to file
+      writeFile(stm, fileSize);
+      System.out.println("File " + file1 + " written to.");
+
+      // close file
+      stm.close();
+      System.out.println("File " + file1 + " closed.");
+
+      // Make sure a client can read it
+      checkFullFile(fs, file1, fileSize);
+
+      // verify that file size has changed
+      long len = fs.getFileStatus(file1).getLen();
+      assertTrue(file1 + " should be of size " +  fileSize +
+                 " but found to be of size " + len,
+                  len == fileSize);
+
+    } finally {
+      cluster.shutdown();
+    }
+  }
+}
-- 
1.7.0.4

